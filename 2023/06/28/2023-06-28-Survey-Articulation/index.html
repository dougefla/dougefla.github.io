<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="A survey for articulation estimationThe articulation estimation could be divided into 3 types based on the input information.According to the input information, the articulation estimation could be di">
<meta property="og:type" content="article">
<meta property="og:title" content="Survey on Articulation Estimation">
<meta property="og:url" content="http://example.com/2023/06/28/2023-06-28-Survey-Articulation/index.html">
<meta property="og:site_name" content="Douge&#39;s Blog">
<meta property="og:description" content="A survey for articulation estimationThe articulation estimation could be divided into 3 types based on the input information.According to the input information, the articulation estimation could be di">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-06-28T12:23:00.000Z">
<meta property="article:modified_time" content="2023-06-30T03:53:29.721Z">
<meta property="article:author" content="Fu Lian">
<meta property="article:tag" content="English">
<meta property="article:tag" content="Research">
<meta property="article:tag" content="Survey">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2023/06/28/2023-06-28-Survey-Articulation/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Survey on Articulation Estimation | Douge's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Douge's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/28/2023-06-28-Survey-Articulation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Fu Lian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Douge's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Survey on Articulation Estimation
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-06-28 21:23:00" itemprop="dateCreated datePublished" datetime="2023-06-28T21:23:00+09:00">2023-06-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-30 12:53:29" itemprop="dateModified" datetime="2023-06-30T12:53:29+09:00">2023-06-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index"><span itemprop="name">Research</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="A-survey-for-articulation-estimation"><a href="#A-survey-for-articulation-estimation" class="headerlink" title="A survey for articulation estimation"></a>A survey for articulation estimation</h1><p>The articulation estimation could be divided into 3 types based on the input information.<br>According to the input information, the articulation estimation could be divided into 3 types.</p>
<ul>
<li>Single Observation Based<ul>
<li>CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects</li>
<li>CA2T-Net: Category-Agnostic 3D Articulation Transfer from Single Image</li>
<li>Shape2Motion: Joint Analysis of Motion Parts and Attributes from 3D Shapes</li>
</ul>
</li>
<li>Multi-Observation Based</li>
</ul>
<h2 id="According-to-how-to-use-the-feature-the-articulation-estimation-could-be-divided-into-2-types-Exact-Model-Based-CA2T-Net-Category-Agnostic-3D-Articulation-Transfer-from-Single-Image-Explict-Model-Based-CARTO-Category-and-Joint-Agnostic-Reconstruction-of-ARTiculated-Objects-Implicit-Model-Based"><a href="#According-to-how-to-use-the-feature-the-articulation-estimation-could-be-divided-into-2-types-Exact-Model-Based-CA2T-Net-Category-Agnostic-3D-Articulation-Transfer-from-Single-Image-Explict-Model-Based-CARTO-Category-and-Joint-Agnostic-Reconstruction-of-ARTiculated-Objects-Implicit-Model-Based" class="headerlink" title="According to how to use the feature, the articulation estimation could be divided into 2 types.- Exact Model Based  - CA2T-Net: Category-Agnostic 3D Articulation Transfer from Single Image- Explict Model Based  - CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects- Implicit Model Based"></a>According to how to use the feature, the articulation estimation could be divided into 2 types.<br>- Exact Model Based<br>  - CA2T-Net: Category-Agnostic 3D Articulation Transfer from Single Image<br>- Explict Model Based<br>  - CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects<br>- Implicit Model Based</h2><h2 id="According-to-the-estimation-method-the-articulation-estimation-could-be-divided-into-2-types-Non-Deeplearning-Methods-Supervised-Deeplearning-Methods-CARTO-Category-and-Joint-Agnostic-Reconstruction-of-ARTiculated-Objects-Semi-Weakly-Supervised-Deeplearning-Methods-Semi-Weakly-Supervised-Object-Kinematic-Motion-Prediction"><a href="#According-to-the-estimation-method-the-articulation-estimation-could-be-divided-into-2-types-Non-Deeplearning-Methods-Supervised-Deeplearning-Methods-CARTO-Category-and-Joint-Agnostic-Reconstruction-of-ARTiculated-Objects-Semi-Weakly-Supervised-Deeplearning-Methods-Semi-Weakly-Supervised-Object-Kinematic-Motion-Prediction" class="headerlink" title="According to the estimation method, the articulation estimation could be divided into 2 types.- Non-Deeplearning Methods- Supervised Deeplearning Methods  - CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects- Semi-Weakly Supervised Deeplearning Methods  - Semi-Weakly Supervised Object Kinematic Motion Prediction"></a>According to the estimation method, the articulation estimation could be divided into 2 types.<br>- Non-Deeplearning Methods<br>- Supervised Deeplearning Methods<br>  - CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects<br>- Semi-Weakly Supervised Deeplearning Methods<br>  - Semi-Weakly Supervised Object Kinematic Motion Prediction</h2><ul>
<li>Non-supervised Deeplearning Methods</li>
</ul>
<p>According to the stages of the articulation estimation, the articulation estimation could be divided into 2 types.</p>
<ul>
<li>Multi Stage<ul>
<li>Shape2Motion: Joint Analysis of Motion Parts and Attributes from 3D Shapes</li>
</ul>
</li>
<li>Single Stage (End-to-End)<ul>
<li>CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects</li>
<li>CA2T-Net: Category-Agnostic 3D Articulation Transfer from Single Image</li>
<li>Shape2Motion: Joint Analysis of Motion Parts and Attributes from 3D Shapes</li>
</ul>
</li>
</ul>
<p>Articulation estimation sometimes combines with object detection, pose estimation, tracking, or reconstruction.</p>
<ul>
<li>Object Detection</li>
<li>Reconstruction<ul>
<li>CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects</li>
<li>SAOR: Single-View Articulated Object Reconstruction</li>
</ul>
</li>
</ul>
<h2 id="Non-Deeplearning-Methods"><a href="#Non-Deeplearning-Methods" class="headerlink" title="Non-Deeplearning Methods"></a>Non-Deeplearning Methods</h2><p>Before deep learning becomes popular, there are some methods focusing on solving this articulation estimation task.</p>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h4 id="CARTO-Category-and-Joint-Agnostic-Reconstruction-of-ARTiculated-Objects"><a href="#CARTO-Category-and-Joint-Agnostic-Reconstruction-of-ARTiculated-Objects" class="headerlink" title="CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects"></a>CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects</h4><h2 id="Deeplearning-Methods"><a href="#Deeplearning-Methods" class="headerlink" title="Deeplearning Methods"></a>Deeplearning Methods</h2><h2 id="Deeplearning-Methods-1"><a href="#Deeplearning-Methods-1" class="headerlink" title="Deeplearning Methods"></a>Deeplearning Methods</h2><h4 id="Semi-Weakly-Supervised-Object-Kinematic-Motion-Prediction"><a href="#Semi-Weakly-Supervised-Object-Kinematic-Motion-Prediction" class="headerlink" title="Semi-Weakly Supervised Object Kinematic Motion Prediction"></a>Semi-Weakly Supervised Object Kinematic Motion Prediction</h4><h4 id="Towards-Understanding-Articulated-Objects"><a href="#Towards-Understanding-Articulated-Objects" class="headerlink" title="Towards Understanding Articulated Objects"></a>Towards Understanding Articulated Objects</h4><p>Robots operating in home environments must be able to interact with articulated objects such as doors or drawers. Ideally, robots are able to autonomously infer articulation models by observation. In this paper, we present an approach to learn kinematic models by inferring the connectivity of rigid parts and the articulation models for the corresponding links. Our method uses a mixture of parameterized and parameter-free representations. To obtain parameter-free models, we seek for low-dimensional manifolds of latent action variables in order to provide the best explanation of the given observations. The mapping from the constrained manifold of an articulated link to the work space is learned by means of Gaussian process regression. Our approach has been implemented and evaluated using real data obtained in various home environment settings. Finally, we discuss the limitations and possible extensions of the proposed method.</p>
<h4 id="Interactive-Segmentation-Tracking-and-Kinematic-Modeling-of-Unknown-Articulated-Objects"><a href="#Interactive-Segmentation-Tracking-and-Kinematic-Modeling-of-Unknown-Articulated-Objects" class="headerlink" title="Interactive Segmentation, Tracking, and Kinematic Modeling of Unknown Articulated Objects"></a>Interactive Segmentation, Tracking, and Kinematic Modeling of Unknown Articulated Objects</h4><p>We present an interactive perceptual skill for segmenting, tracking, and kinematic modeling of 3D articulated objects. This skill is a prerequisite for general manipulation in unstructured environments. Robot-environment interaction is used to move an unknown object, creating a perceptual signal that reveals the kinematic properties of the object. The resulting perceptual information can then inform and facilitate further manipulation. The algorithm is computationally efficient, handles occlusion, and depends on little object motion; it only requires sufficient texture for visual feature tracking. We conducted experiments with everyday objects on a mobile manipulation platform equipped with an RGB-D sensor. The results demonstrate the robustness of the proposed method to lighting conditions, object appearance, size, structure, and configuration.</p>
<h4 id="Reconstructing-Articulated-Rigged-Models-from-RGB-D-Videos"><a href="#Reconstructing-Articulated-Rigged-Models-from-RGB-D-Videos" class="headerlink" title="Reconstructing Articulated Rigged Models from RGB-D Videos"></a>Reconstructing Articulated Rigged Models from RGB-D Videos</h4><p>Although commercial and open-source software exist to reconstruct a static object from a sequence recorded with an RGB-D sensor, there is a lack of tools that build rigged models of articulated objects that deform realistically and can be used for tracking or animation. In this work, we fill this gap and propose a method that creates a fully rigged model of an articulated object from depth data of a single sensor. To this end, we combine deformable mesh tracking, motion segmentation based on spectral clustering and skeletonization based on mean curvature flow. The fully rigged model then consists of a watertight mesh, embedded skeleton, and skinning weights.</p>
<h4 id="The-RBO-Dataset-of-Articulated-Objects-and-Interactions"><a href="#The-RBO-Dataset-of-Articulated-Objects-and-Interactions" class="headerlink" title="The RBO Dataset of Articulated Objects and Interactions"></a>The RBO Dataset of Articulated Objects and Interactions</h4><p>We present a dataset with models of 14 articulated objects commonly found in human environments and with RGBD video sequences and wrenches recorded of human interactions with them. The 358 interaction sequences total 67 minutes of human manipulation under varying experimental conditions (type of interaction, lighting, perspective, and background). Each interaction with an object is annotated with the ground truth poses of its rigid parts and the kinematic state obtained by a motion capture system. For a subset of 78 sequences (25 minutes), we also measured the interaction wrenches. The object models contain textured three-dimensional triangle meshes of each link and their motion constraints. We provide Python scripts to download and visualize the data.</p>
<h4 id="Deep-Part-Induction-from-Articulated-Object-Pairs"><a href="#Deep-Part-Induction-from-Articulated-Object-Pairs" class="headerlink" title="Deep Part Induction from Articulated Object Pairs"></a>Deep Part Induction from Articulated Object Pairs</h4><p>Object functionality is often expressed through part articulation – as when the two rigid parts of a scissor pivot against each other to perform the cutting function. Such articulations are often similar across objects within the same functional category. In this paper we explore how the observation of different articulation states provides evidence for part structure and motion of 3D objects. Our method takes as input a pair of unsegmented shapes representing two different articulation states of two functionally related objects, and induces their common parts along with their underlying rigid motion. This is a challenging setting, as we assume no prior shape structure, no prior shape category information, no consistent shape orientation, the articulation states may belong to objects of different geometry, plus we allow inputs to be noisy and partial scans, or point clouds lifted from RGB images. Our method learns a neural network architecture with three modules that respectively propose correspondences, estimate 3D deformation flows, and perform segmentation. To achieve optimal performance, our architecture alternates between correspondence, deformation flow, and segmentation prediction iteratively in an ICP-like fashion. Our results demonstrate that our method significantly outperforms state-of-the-art techniques in the task of discovering articulated parts of objects. In addition, our part induction is object-class agnostic and successfully generalizes to new and unseen objects.</p>
<h4 id="Deep-Learning-Based-Robotic-Tool-Detection-and-Articulation-Estimation-With-Spatio-Temporal-Layers"><a href="#Deep-Learning-Based-Robotic-Tool-Detection-and-Articulation-Estimation-With-Spatio-Temporal-Layers" class="headerlink" title="Deep Learning Based Robotic Tool Detection and Articulation Estimation With Spatio-Temporal Layers"></a>Deep Learning Based Robotic Tool Detection and Articulation Estimation With Spatio-Temporal Layers</h4><p>Surgical-tool joint detection from laparoscopic images is an important but challenging task in computer-assisted minimally invasive surgery. Illumination levels, variations in background and the different number of tools in the field of view, all pose difficulties to algorithm and model training. Yet, such challenges could be potentially tackled by exploiting the temporal information in laparoscopic videos to avoid per frame handling of the problem. In this letter, we propose a novel encoder–decoder architecture for surgical instrument joint detection and localization that uses three-dimensional convolutional layers to exploit spatio-temporal features from laparoscopic videos. When tested on benchmark and custom-built datasets, a median Dice similarity coefficient of 85.1% with an interquartile range of 4.6% highlights performance better than the state of the art based on single-frame processing. Alongside novelty of the network architecture, the idea for inclusion of temporal information appears to be particularly useful when processing images with unseen backgrounds during the training phase, which indicates that spatio-temporal features for joint detection help to generalize the solution.</p>
<h4 id="Learning-to-Generalize-Kinematic-Models-to-Novel-Objects"><a href="#Learning-to-Generalize-Kinematic-Models-to-Novel-Objects" class="headerlink" title="Learning to Generalize Kinematic Models to Novel Objects"></a>Learning to Generalize Kinematic Models to Novel Objects</h4><p>Robots operating in human environments must be capable of interacting with a wide variety of articulated objects such as cabinets, refrigerators, and drawers. Existing approaches require human demonstration or minutes of interaction to fit kinematic models to each novel object from scratch. We present a framework for estimating the kinematic model and configuration of previously unseen articulated objects, conditioned upon object type, from as little as a single observation. We train our system in simulation with a novel dataset of synthetic articulated objects; at runtime, our model can predict the shape and kinematic model of an object from depth sensor data. We demonstrate that our approach enables a MOVO robot to view an object with its RGB-D sensor, estimate its motion model, and use that estimate to interact with the object.</p>
<h4 id="RPM-Net-Recurrent-Prediction-of-Motion-and-Parts-from-Point-Cloud"><a href="#RPM-Net-Recurrent-Prediction-of-Motion-and-Parts-from-Point-Cloud" class="headerlink" title="RPM-Net: Recurrent Prediction of Motion and Parts from Point Cloud"></a>RPM-Net: Recurrent Prediction of Motion and Parts from Point Cloud</h4><p>We introduce RPM-Net, a deep learning-based approach which simultaneously infers movable parts and hallucinates their motions from a single, un-segmented, and possibly partial, 3D point cloud shape. RPM-Net is a novel Recurrent Neural Network (RNN), composed of an encoder-decoder pair with interleaved Long Short-Term Memory (LSTM) components, which together predict a temporal sequence of pointwise displacements for the input point cloud. At the same time, the displacements allow the network to learn movable parts, resulting in a motion-based shape segmentation. Recursive applications of RPM-Net on the obtained parts can predict finer-level part motions, resulting in a hierarchical object segmentation. Furthermore, we develop a separate network to estimate part mobilities, e.g., per-part motion parameters, from the segmented motion sequence. Both networks learn deep predictive models from a training set that exemplifies a variety of mobilities for diverse objects. We show results of simultaneous motion and part predictions from synthetic and real scans of 3D objects exhibiting a variety of part mobilities, possibly involving multiple movable parts.</p>
<h4 id="A-Hand-Motion-guided-Articulation-and-Segmentation-Estimation"><a href="#A-Hand-Motion-guided-Articulation-and-Segmentation-Estimation" class="headerlink" title="A Hand Motion-guided Articulation and Segmentation Estimation"></a>A Hand Motion-guided Articulation and Segmentation Estimation</h4><p>we present a method for simultaneous articulation model estimation and segmentation of an articulated object in RGB-D images using human hand motion. Our method uses the hand motion in the processes of the initial articulation model estimation, ICP-based model parameter optimization, and region selection of the target object. The hand motion gives an initial guess of the articulation model: prismatic or revolute joint. The method estimates the joint parameters by aligning the RGB-D images with the constraint of the hand motion. Finally, the target regions are selected from the cluster regions which move symmetrically along with the articulation model. Our experimental results show the robustness of the proposed method for the various objects.</p>
<h4 id="Category-Level-Articulated-Object-Pose-Estimation"><a href="#Category-Level-Articulated-Object-Pose-Estimation" class="headerlink" title="Category-Level Articulated Object Pose Estimation"></a>Category-Level Articulated Object Pose Estimation</h4><p>This paper addresses the task of category-level pose estimation for articulated objects from a single depth image. We present a novel category-level approach that correctly accommodates object instances previously unseen during training. We introduce Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH) – a canonical representation for different articulated objects in a given category. As the key to achieve intra-category generalization, the representation constructs a canonical object space as well as a set of canonical part spaces. The canonical object space normalizes the object orientation, scales and articulations (e.g. joint parameters and states) while each canonical part space further normalizes its part pose and scale. We develop a deep network based on PointNet++ that predicts ANCSH from a single depth point cloud, including part segmentation, normalized coordinates, and joint parameters in the canonical object space. By leveraging the canonicalized joints, we demonstrate: (1) improved performance in part pose and scale estimations using the induced kinematic constraints from joints; (2) high accuracy for joint parameter estimation in camera space.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/English/" rel="tag"># English</a>
              <a href="/tags/Research/" rel="tag"># Research</a>
              <a href="/tags/Survey/" rel="tag"># Survey</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/06/13/2023-06-13-How-to-cook-Chiffon-Cake/" rel="prev" title="How to Cook Chocolate Chiffon Cake / チョコレートシフォンケーキの作り方">
      <i class="fa fa-chevron-left"></i> How to Cook Chocolate Chiffon Cake / チョコレートシフォンケーキの作り方
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/08/24/2023-08-24-Save-Your-VPS-From-GFW/" rel="next" title="How to save your VPS from GFW">
      How to save your VPS from GFW <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#A-survey-for-articulation-estimation"><span class="nav-number">1.</span> <span class="nav-text">A survey for articulation estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#According-to-how-to-use-the-feature-the-articulation-estimation-could-be-divided-into-2-types-Exact-Model-Based-CA2T-Net-Category-Agnostic-3D-Articulation-Transfer-from-Single-Image-Explict-Model-Based-CARTO-Category-and-Joint-Agnostic-Reconstruction-of-ARTiculated-Objects-Implicit-Model-Based"><span class="nav-number">1.1.</span> <span class="nav-text">According to how to use the feature, the articulation estimation could be divided into 2 types.- Exact Model Based  - CA2T-Net: Category-Agnostic 3D Articulation Transfer from Single Image- Explict Model Based  - CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects- Implicit Model Based</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#According-to-the-estimation-method-the-articulation-estimation-could-be-divided-into-2-types-Non-Deeplearning-Methods-Supervised-Deeplearning-Methods-CARTO-Category-and-Joint-Agnostic-Reconstruction-of-ARTiculated-Objects-Semi-Weakly-Supervised-Deeplearning-Methods-Semi-Weakly-Supervised-Object-Kinematic-Motion-Prediction"><span class="nav-number">1.2.</span> <span class="nav-text">According to the estimation method, the articulation estimation could be divided into 2 types.- Non-Deeplearning Methods- Supervised Deeplearning Methods  - CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects- Semi-Weakly Supervised Deeplearning Methods  - Semi-Weakly Supervised Object Kinematic Motion Prediction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Non-Deeplearning-Methods"><span class="nav-number">1.3.</span> <span class="nav-text">Non-Deeplearning Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.1.</span> <span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CARTO-Category-and-Joint-Agnostic-Reconstruction-of-ARTiculated-Objects"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deeplearning-Methods"><span class="nav-number">1.4.</span> <span class="nav-text">Deeplearning Methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deeplearning-Methods-1"><span class="nav-number">1.5.</span> <span class="nav-text">Deeplearning Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Semi-Weakly-Supervised-Object-Kinematic-Motion-Prediction"><span class="nav-number">1.5.0.1.</span> <span class="nav-text">Semi-Weakly Supervised Object Kinematic Motion Prediction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Towards-Understanding-Articulated-Objects"><span class="nav-number">1.5.0.2.</span> <span class="nav-text">Towards Understanding Articulated Objects</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Interactive-Segmentation-Tracking-and-Kinematic-Modeling-of-Unknown-Articulated-Objects"><span class="nav-number">1.5.0.3.</span> <span class="nav-text">Interactive Segmentation, Tracking, and Kinematic Modeling of Unknown Articulated Objects</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reconstructing-Articulated-Rigged-Models-from-RGB-D-Videos"><span class="nav-number">1.5.0.4.</span> <span class="nav-text">Reconstructing Articulated Rigged Models from RGB-D Videos</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-RBO-Dataset-of-Articulated-Objects-and-Interactions"><span class="nav-number">1.5.0.5.</span> <span class="nav-text">The RBO Dataset of Articulated Objects and Interactions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-Part-Induction-from-Articulated-Object-Pairs"><span class="nav-number">1.5.0.6.</span> <span class="nav-text">Deep Part Induction from Articulated Object Pairs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-Learning-Based-Robotic-Tool-Detection-and-Articulation-Estimation-With-Spatio-Temporal-Layers"><span class="nav-number">1.5.0.7.</span> <span class="nav-text">Deep Learning Based Robotic Tool Detection and Articulation Estimation With Spatio-Temporal Layers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-to-Generalize-Kinematic-Models-to-Novel-Objects"><span class="nav-number">1.5.0.8.</span> <span class="nav-text">Learning to Generalize Kinematic Models to Novel Objects</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RPM-Net-Recurrent-Prediction-of-Motion-and-Parts-from-Point-Cloud"><span class="nav-number">1.5.0.9.</span> <span class="nav-text">RPM-Net: Recurrent Prediction of Motion and Parts from Point Cloud</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-Hand-Motion-guided-Articulation-and-Segmentation-Estimation"><span class="nav-number">1.5.0.10.</span> <span class="nav-text">A Hand Motion-guided Articulation and Segmentation Estimation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Category-Level-Articulated-Object-Pose-Estimation"><span class="nav-number">1.5.0.11.</span> <span class="nav-text">Category-Level Articulated Object Pose Estimation</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Fu Lian</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/dougefla" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;dougefla" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.instagram.com/dougefu" title="Instagram → https:&#x2F;&#x2F;www.instagram.com&#x2F;dougefu" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.douban.com/people/191083526" title="Douban → https:&#x2F;&#x2F;www.douban.com&#x2F;people&#x2F;191083526" rel="noopener" target="_blank"><i class="fa custom douban fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/16534751" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;16534751" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Friends
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://kagurua.com/" title="http:&#x2F;&#x2F;kagurua.com" rel="noopener" target="_blank">Mio's Blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://nephritebin.github.io/" title="https:&#x2F;&#x2F;nephritebin.github.io&#x2F;" rel="noopener" target="_blank">Land of the Lustrous</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fu Lian</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'b9f74eae6a09c933df0a',
      clientSecret: '110f9ab37c8468859063a57653f2a054dd3cc777',
      repo        : 'dougefla.github.io',
      owner       : 'dougefla',
      admin       : ['dougefla'],
      id          : '0b0909a25a599d476887942f8931c45a',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
